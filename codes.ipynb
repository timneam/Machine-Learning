{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the statistical description in box plot\n",
    "heart_dataset.plot(kind='box', subplots=True, layout=(30,30), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing dataset in histogram graph\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "ax = fig.gca()\n",
    "heart_dataset.hist(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the correlation between \n",
    "plt.matshow(heart_dataset.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = heart_dataset.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data = pd.DataFrame({'Age':[0,2,4,13,35,-1,54]})\n",
    "\n",
    "bins= [0,2,4,13,20,110]\n",
    "labels = ['Infant','Toddler','Kid','Teen','Adult']\n",
    "X_train_data['AgeGroup'] = pd.cut(X_train_data['Age'], bins=bins, labels=labels, right=False)\n",
    "print (X_train_data)\n",
    "   Age AgeGroup\n",
    "0    0   Infant\n",
    "1    2  Toddler\n",
    "2    4      Kid\n",
    "3   13     Teen\n",
    "4   35    Adult\n",
    "5   -1      NaN\n",
    "6   54    Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "df = pd.read_csv(\"heart_failure_clinical_records_dataset.csv\")\n",
    "\n",
    "#remove the fields from the data set that we don't want to include in our model\n",
    "del df[\"house_number\"]\n",
    "del df[\"unit_number\"]\n",
    "del df[\"street_name\"]\n",
    "del df[\"zip_code\"]\n",
    "\n",
    "#replace categorical data with one-hot encoded data\n",
    "features_df = pd.get_dummies(df, columns=['garage_type', 'city'])\n",
    "\n",
    "#remove the sale price from the feature data\n",
    "del features_df['sale_price']\n",
    "\n",
    "#create the x and y arrays\n",
    "x = features_df.values\n",
    "y = df['sale_price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data set in a training set (70%) and a test set (30%)\n",
    "x_train, x_test, y_train,y_test = train_test_split(x,y,test_size=0.3, random_state=7)\n",
    "\n",
    "#fit regression model\n",
    "model = ensemble.GradientBoostingRegressor(\n",
    "    n_estimators=1000, #how many decision trees to build\n",
    "    learning_rate=0.1, #how much decision trees influence overall preediction\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=9,\n",
    "    max_features=0.1,\n",
    "    loss='huber',\n",
    "    random_state=7\n",
    ")\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "#save the trained moedl to a file so we can use it in ohter programm\n",
    "joblib.dump(model, 'models//trained_house_classifier_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=200)))\n",
    "models.append(('LR', LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=200)))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)       \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=20,8\n",
    "sns.set_style('darkgrid')\n",
    "ax = sns.barplot(x=model_list, y=model_accuracy_list, palette = \"husl\", saturation =2.0)\n",
    "plt.xlabel('Classifier Models', fontsize = 20 )\n",
    "plt.ylabel('% of Accuracy', fontsize = 20)\n",
    "plt.title('Accuracy of different Classifier Models', fontsize = 20)\n",
    "plt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\n",
    "plt.yticks(fontsize = 12)\n",
    "for i in ax.patches:\n",
    "    width, height = i.get_width(), i.get_height()\n",
    "    x, y = i.get_xy() \n",
    "    ax.annotate(f'{round(height,2)}%', (x + width/2, y + height*1.02), ha='center', fontsize = 'x-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data set in a training set (70%) and a test set (30%)\n",
    "x_train, x_test, y_train,y_test = train_test_split(x,y,test_size=0.3, random_state=7)\n",
    "\n",
    "# Test options and evaluation metric\n",
    "param_grid = {\n",
    "    'seed': [1,3,5,7,9],\n",
    "    'scoring':'accuracy',\n",
    "    'n_estimators': [500, 1000, 3000], #how many decision trees to build\n",
    "    'max_depth': [4, 6],\n",
    "    'min_samples_leaf': [3, 5, 9, 17],\n",
    "    'learning_rate': [0.1, 0.05, 0.02, 0.01], #how much decision trees influence overall preediction\n",
    "    'max_features': [1.0, 0.3, 0.1],\n",
    "    'loss': ['ls', 'lad', 'huber']\n",
    "}\n",
    "\n",
    "# Spot Check Algorithms\n",
    "LR = LogisticRegression()\n",
    "KNN = KNeighborsClassifier()\n",
    "CART = DecisionTreeClassifier()\n",
    "GBC = GradientBoostingClassifier()\n",
    "RFC = RandomForestClassifier()\n",
    "XGBRF = xgboost.XGBRFClassifier()\n",
    "LGBM = lightgbm.LGBMClassifier()\n",
    "NB = GaussianNB()\n",
    "\n",
    "model1 = GridSearchCV(LR, param_grid, n_jobs=4, verbose=100)\n",
    "model2 = GridSearchCV(KNN, param_grid, n_jobs=4, verbose=100)\n",
    "model3= GridSearchCV(CART, param_grid, n_jobs=4, verbose=100)\n",
    "model4 = GridSearchCV(BGC, param_grid, n_jobs=4, verbose=100)\n",
    "model5 = GridSearchCV(RFC, param_grid, n_jobs=4, verbose=100)\n",
    "model6 = GridSearchCV(XGBRF, param_grid, n_jobs=4, verbose=100)\n",
    "model7 = GridSearchCV(LGBM, param_grid, n_jobs=4, verbose=100)\n",
    "model8 = GridSearchCV(NB, param_grid, n_jobs=4, verbose=100)\n",
    "model9 = GridSearchCV(XVC, param_grid, n_jobs=4, verbose=100)\n",
    "\n",
    "# Run the grid search - on only the training data!\n",
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "model3.fit(X_train, y_train)\n",
    "model4.fit(X_train, y_train)\n",
    "model5.fit(X_train, y_train)\n",
    "model6.fit(X_train, y_train)\n",
    "model7.fit(X_train, y_train)\n",
    "model8.fit(X_train, y_train)\n",
    "model9.fit(X_train, y_train)\n",
    "\n",
    "# Print the parameters that gave us the best result!\n",
    "print(model1.best_params_)\n",
    "print(model2.best_params_)\n",
    "print(model3.best_params_)\n",
    "print(model4.best_params_)\n",
    "print(model5.best_params_)\n",
    "print(model6.best_params_)\n",
    "print(model7.best_params_)\n",
    "print(model8.best_params_)\n",
    "print(model9.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data set in a training set (70%) and a test set (30%)\n",
    "x_train, x_test, y_train,y_test = train_test_split(x,y,test_size=0.3, random_state=7)\n",
    "\n",
    "# Test options and evaluation metric\n",
    "param_grid = {\n",
    "    'seed': 7,\n",
    "    'scoring':'accuracy',\n",
    "    'n_estimators': 1000, #how many decision trees to build\n",
    "    'learning_rate': 0.2, #how much decision trees influence overall preediction\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 50,\n",
    "    'max_features': 'sqrt',\n",
    "    'loss':'huber'\n",
    "}\n",
    "\n",
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=200)))\n",
    "models.append(('LR', LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=200)))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('GBC', GradientBoostingClassifier()))\n",
    "models.append(('rfc', RandomForestClassifier()))\n",
    "models.append(('xgbrf', xgboost.XGBRFClassifier()))\n",
    "models.append(('lightgbm', lightgbm.LGBMClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)       \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the feature importance table, the top 5 feature selected are:\n",
    "# age, ejection fraction, Serum creatinine, Serum sodium, time and death event\n",
    "Features = ['age','time','ejection_fraction','serum_creatinine', 'serum_sodium']\n",
    "\n",
    "x = heart_dataset[Features]\n",
    "y = heart_dataset[\"DEATH_EVENT\"]\n",
    "\n",
    "#split the data set in a training set (70%) and a test set (30%)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binning of 3 columns: age, ejection fraction and time\n",
    "df = pd.read_csv(\"heart_failure_clinical_records_dataset.csv\")\n",
    "df['binned_age'] = pd.cut(heart_dataset['age'], bins = [40.0,50.0,60.0,70.0,80.0,90.0,100.0])\n",
    "\n",
    "df['binned_ejection_fraction'] = pd.cut(heart_dataset['ejection_fraction'], bins = [0{0 - 40}, 1{41 - 49}, 2{50 -70}])\n",
    "\n",
    "df['binned_time'] = pd.cut(heart_dataset['time'], bins = [0{0.0 - 50.0}, 1{51 - 100}, 2{101 - 150}, 3{151 - 200}, 4{201 - 250}, 5{251 - 300}])\n",
    "\n",
    "df['binned_serum_sodium'] = pd.cut(heart_dataset['time'], bins = [100.0,134.0,135.0,145.0,200.0])\n",
    "\n",
    "\n",
    "df.to_csv('new_heart_failure_clinical_records_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "x = heart_dataset.iloc[:, :-1]\n",
    "y = heart_dataset.iloc[:,-1]\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(x,y)\n",
    "\n",
    "print(model.feature_importances_) \n",
    "\n",
    "feat_importances = pd.Series(model.feature_importances_, index=x.columns)\n",
    "feat_importances.nlargest(12).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train, y_train)\n",
    "\n",
    "log_reg_pred = log_reg.predict(x_test)\n",
    "\n",
    "log_reg_acc = accuracy_score(y_test, log_reg_pred)\n",
    "\n",
    "model_list.append('log_reg')\n",
    "model_accuracy_list.append(100*log_reg_acc)\n",
    "\n",
    "#save the trained moedl to a file so we can use it in ohter programm\n",
    "joblib.dump(log_reg, 'logistic_regression_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support vector classifier\n",
    "\n",
    "sv_clf = SVC()\n",
    "sv_clf.fit(x_train, y_train)\n",
    "\n",
    "sv_clf_pred = sv_clf.predict(x_test)\n",
    "\n",
    "sv_clf_acc = accuracy_score(y_test, sv_clf_pred)\n",
    "\n",
    "model_list.append('sv_clf')\n",
    "model_accuracy_list.append(100* sv_clf_acc)\n",
    "\n",
    "#save the trained moedl to a file so we can use it in ohter programm\n",
    "joblib.dump(sv_clf, 'logistic_regression_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Neighbors Classifier\n",
    "\n",
    "kn_clf = KNeighborsClassifier(n_neighbors=6)\n",
    "kn_clf.fit(x_train, y_train)\n",
    "\n",
    "kn_pred = kn_clf.predict(x_test)\n",
    "\n",
    "kn_acc = accuracy_score(y_test, kn_pred)\n",
    "\n",
    "model_list.append('kn_clf')\n",
    "model_accuracy_list.append(100*kn_acc)\n",
    "\n",
    "#save the trained moedl to a file so we can use it in ohter programm\n",
    "joblib.dump(kn_clf, 'logistic_regression_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0, criterion='entropy')\n",
    "dt_clf.fit(x_train, y_train)\n",
    "\n",
    "dt_pred = dt_clf.predict(x_test)\n",
    "\n",
    "dt_acc = accuracy_score(y_test, dt_pred)\n",
    "\n",
    "model_list.append('dt_clf')\n",
    "model_accuracy_list.append(100*dt_acc)\n",
    "\n",
    "#save the trained moedl to a file so we can use it in ohter programm\n",
    "joblib.dump(dt_clf, 'logistic_regression_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier\n",
    "\n",
    "gradientboost_clf = GradientBoostingClassifier(max_depth=2, random_state=1)\n",
    "gradientboost_clf.fit(x_train,y_train)\n",
    "\n",
    "gradientboost_pred = gradientboost_clf.predict(x_test)\n",
    "\n",
    "gradientboost_acc = accuracy_score(y_test, gradientboost_pred)\n",
    "\n",
    "model_list.append('gradientboost_clf')\n",
    "model_accuracy_list.append(100*gradientboost_acc)\n",
    "\n",
    "#save the trained moedl to a file so we can use it in ohter programm\n",
    "joblib.dump(gradientboost_clf, 'logistic_regression_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "\n",
    "r_clf = RandomForestClassifier(max_features=0.5, max_depth=15, random_state=1)\n",
    "r_clf.fit(x_train, y_train)\n",
    "\n",
    "r_pred = r_clf.predict(x_test)\n",
    "\n",
    "r_acc = accuracy_score(y_test, r_pred)\n",
    "\n",
    "model_list.append('r_clf')\n",
    "model_accuracy_list.append(100*r_acc)\n",
    "\n",
    "#save the trained moedl to a file so we can use it in ohter programm\n",
    "joblib.dump(r_clf, 'logistic_regression_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgbrf classifier\n",
    "\n",
    "xgb_clf = xgboost.XGBRFClassifier(max_depth=3, random_state=1)\n",
    "xgb_clf.fit(x_train,y_train)\n",
    "\n",
    "xgb_pred = xgb_clf.predict(x_test)\n",
    "\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "\n",
    "model_list.append('xgb_clf')\n",
    "model_accuracy_list.append(100*xgb_acc)\n",
    "\n",
    "#save the trained moedl to a file so we can use it in ohter programm\n",
    "joblib.dump(xgb_clf, 'logistic_regression_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  lightgbm \n",
    "\n",
    "lgb_clf = lightgbm.LGBMClassifier(max_depth=2, random_state=4)\n",
    "lgb_clf.fit(x_train,y_train)\n",
    "\n",
    "lgb_pred = lgb_clf.predict(x_test)\n",
    "\n",
    "lgb_acc = accuracy_score(y_test, lgb_pred)\n",
    "\n",
    "model_list.append('lgb_clf')\n",
    "model_accuracy_list.append(100*lgb_acc)\n",
    "\n",
    "#save the trained moedl to a file so we can use it in ohter programm\n",
    "joblib.dump(lgb_clf, 'logistic_regression_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
